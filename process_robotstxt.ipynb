{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc84bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from advertools import robotstxt_to_df, robotstxt_test\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from protego import Protego\n",
    "\n",
    "import requests\n",
    "\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ab1511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robotstxt_to_df_modified(robots_read):\n",
    "    robots_text = robots_read.splitlines()\n",
    "    lines = []\n",
    "    for line in robots_text:\n",
    "        if line.strip():\n",
    "            if line.strip().startswith('#'):\n",
    "                lines.append(['comment',\n",
    "                                (line.replace('#', '').strip())])\n",
    "            else:\n",
    "                split = line.split(':', maxsplit=1)\n",
    "                lines.append([split[0].strip(), split[1].strip()])\n",
    "    df = pd.DataFrame(lines, columns=['directive', 'content'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "162d6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robotstxt_test_modified(robots_text, robotstxt_url, user_agents, urls):\n",
    "    if not robotstxt_url.endswith('/robots.txt'):\n",
    "        raise ValueError('Please make sure you enter a valid robots.txt URL')\n",
    "    if isinstance(user_agents, str):\n",
    "        user_agents = [user_agents]\n",
    "    if isinstance(urls, str):\n",
    "        urls = [urls]\n",
    "        \n",
    "    rp = Protego.parse(robots_text)\n",
    "    test_list = []\n",
    "    for path, agent in product(urls, user_agents):\n",
    "        d = dict()\n",
    "        d['user_agent'] = agent\n",
    "        d['url_path'] = path\n",
    "        d['can_fetch'] = rp.can_fetch(path, agent)\n",
    "        test_list.append(d)\n",
    "    df = pd.DataFrame(test_list)\n",
    "    df.insert(0, 'robotstxt_url', robotstxt_url)\n",
    "    df = df.sort_values(['user_agent', 'url_path']).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9ad1235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bias_from_robotstxt_file(robots_text, robotstxt_url):\n",
    "    '''\n",
    "    input: robots.txt url\n",
    "    output: return 3 lists: no_bias bots list, favored bots list, disfavored bots list\n",
    "    \n",
    "    Example input: https://www.nytimes.com/robots.txt\n",
    "    output : \n",
    "    user_agent\n",
    "    *                 no_bias\n",
    "    Twitterbot        favored\n",
    "    ia_archiver    disfavored\n",
    "    omgili         disfavored\n",
    "    omgilibot      disfavored\n",
    "    dtype: object\n",
    "    '''\n",
    "    # robots.txt file to pd dataframe \n",
    "    df = robotstxt_to_df_modified(robots_text)\n",
    "    \n",
    "    # get all user agents in a list\n",
    "    user_agents = df.loc[df['directive'].str.lower() == 'user-agent']['content'].drop_duplicates().to_list()\n",
    "    \n",
    "    #initiate return lists\n",
    "    no_bias, fav, disfav, fav_count_diff, disfav_count_diff = [], [], [], [], []\n",
    "    \n",
    "    # Check for empty robots.txt\n",
    "    if not user_agents or user_agents == ['*']:\n",
    "        return no_bias, fav, disfav, fav_count_diff, disfav_count_diff\n",
    "    \n",
    "    # get all directories that appear in the robots.txt file\n",
    "    urls = df.loc[df['directive'].str.lower().isin(['disallow', 'allow'])]['content'].drop_duplicates().to_list()\n",
    "    \n",
    "    # return a DataFrame with a row for each combination of (user-agent, URL) indicating whether or not that particular user-agent can fetch the given URL.\n",
    "    test = robotstxt_test_modified(robots_text, robotstxt_url, user_agents, urls)\n",
    "    \n",
    "    # return a pd series with user_agent as index and how many urls is allowed to fetch as value\n",
    "    count_allow_by_user_agent = test.groupby(by='user_agent').apply(lambda row: row['can_fetch'].sum())\n",
    "\n",
    "    # check if '*' is in user_agents, else add it to the list with count 0\n",
    "    if not '*' in count_allow_by_user_agent.index:\n",
    "        count_allow_by_user_agent['*'] = 0\n",
    "    \n",
    "    # Number of directories allowed for '*' to use it as reference next\n",
    "    ref = count_allow_by_user_agent['*']\n",
    "\n",
    "    #Create a dataframe from the series\n",
    "    count_allow_by_user_agent_df = pd.DataFrame({'user_agent': count_allow_by_user_agent.index, 'url_can_fetch_count':count_allow_by_user_agent.values})\n",
    "    \n",
    "    def transform_can_fetch_into_categories(row):\n",
    "        v = row.url_can_fetch_count - ref\n",
    "        if v == 0:\n",
    "            bias = 'no_bias'\n",
    "        elif v > 0:\n",
    "            bias = 'favored'\n",
    "        else:\n",
    "            bias = 'disfavored'\n",
    "            \n",
    "        row['bias'] = bias\n",
    "        row['can_fetch_compared_to_*'] = v\n",
    "        \n",
    "        return row\n",
    "        \n",
    "        \n",
    "    # map counts into one of the 3 categories\n",
    "    count_allow_by_user_agent_df = count_allow_by_user_agent_df.apply(transform_can_fetch_into_categories, axis=1)\n",
    "    \n",
    "    #drop user_agent == *\n",
    "    count_allow_by_user_agent_df.drop(count_allow_by_user_agent_df[count_allow_by_user_agent_df.user_agent == '*'].index, inplace=True)\n",
    "    \n",
    "    for index, row in count_allow_by_user_agent_df.iterrows():\n",
    "        if row['bias'] == 'no_bias':\n",
    "            no_bias.append(row['user_agent'])\n",
    "        elif row['bias'] == 'favored':\n",
    "            fav.append(row['user_agent'])\n",
    "            fav_count_diff.append(row['can_fetch_compared_to_*'])\n",
    "        else:\n",
    "            disfav.append(row['user_agent'])\n",
    "            disfav_count_diff.append(row['can_fetch_compared_to_*'])\n",
    "    \n",
    "    # return the final output\n",
    "    return no_bias, fav, disfav, fav_count_diff, disfav_count_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d230237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(data, csv_file_path):\n",
    "    try:\n",
    "        df = pd.DataFrame(data, columns=['robotstxt_url', 'no_bias', 'favored', 'disfavored', 'fav_count_diff', 'disfav_count_diff'])\n",
    "        print('Saving data to: ' + csv_file_path)\n",
    "        df.to_csv(csv_file_path, mode='a', header=False)\n",
    "        return\n",
    "    except:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c30b6e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_robotstxt_file(file_path, batch_size=1024):\n",
    "    data = []\n",
    "    csv_output_path = '/'.join(file_path.split('/')[:-1]) + '/output.csv' \n",
    "    with open(file_path, 'rb') as stream:\n",
    "        for record in ArchiveIterator(stream):\n",
    "            if record.rec_type == 'response' and record.http_headers.get_statuscode() == '200':\n",
    "                txt = record.content_stream().read().decode('ISO-8859-1').replace('ยก', ' ')\n",
    "                robotstxt_url = record.rec_headers.get_header('WARC-Target-URI')\n",
    "                try:\n",
    "                    no_bias, fav, disfav, fav_count_diff, disfav_count_diff = get_bias_from_robotstxt_file(txt, robotstxt_url)\n",
    "                    data.append([robotstxt_url, no_bias, fav, disfav, fav_count_diff, disfav_count_diff])\n",
    "                except:\n",
    "                    pass\n",
    "        if len(data) >= batch_size:\n",
    "            append_to_csv(data, csv_output_path)\n",
    "            data = []\n",
    "    \n",
    "    append_to_csv(data, csv_output_path)\n",
    "    data = []\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c37c7540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "my_path = '/home/osama/CC-MAIN-2021-43-robotstxt/1634323583423.96'\n",
    "files = glob.glob(my_path + '/**/*.gz', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f35fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm_notebook(files):\n",
    "    process_robotstxt_file(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
